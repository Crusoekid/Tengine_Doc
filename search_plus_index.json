{"./":{"url":"./","title":"介绍","keywords":"","body":"Tengine-Lite Tengine Lite 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目基于原有 Tengine 项目使用 C 语言进行重构，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署。同时兼容 Tengine 框架原有 API 和 模型格式 tmfile，降低评估、迁移成本。 "},"Introduction/about.html":{"url":"Introduction/about.html","title":"关于","keywords":"","body":"关于 Tengine Lite 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目基于原有 Tengine 项目使用 C 语言进行重构，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署。同时兼容 Tengine 框架原有 API 和 模型格式 tmfile，降低评估、迁移成本。 Tengine Lite 核心代码由 4 个模块组成： dev：NN Operators 后端模块，当前提供 CPU 代码，后续逐步开源 GPU、NPU 参考代码； lib：框架核心部件，包括 NNIR、计算图、硬件资源、模型解析器的调度和执行模块； op：NN Operators 前端模块，实现 NN Operators 注册、初始化； serializer：模型解析器，实现 tmfile 格式的网络模型参数解析。 "},"Introduction/feedback.html":{"url":"Introduction/feedback.html","title":"反馈","keywords":"","body":"反馈 提交Issue 你可以提交issue给我们。 你不仅可以谈论还能赚钱哦 赏金任务第一期 赏金任务第二期 加入群聊 QQ群（集中地）： 微信群： "},"Introduction/example.html":{"url":"Introduction/example.html","title":"示例","keywords":"","body":"Demo展示 以下所有模型在Tengine model zoo可以找到（密码：hhgc）。 1. 人脸关键点 代码地址：https://github.com/OAID/Tengine/blob/tengine-lite/examples/tm_landmark.cpp export LD_LIBRARY_PATH=./build/install/lib ./build/install/bin/tm_landmark -m models/landmark.tmfile -i images/mobileface02.jpg -r 1 -t 1 Input Image: Output Image: 2. ssd目标检测任务 代码地址：https://github.com/OAID/Tengine/blob/tengine-lite/examples/tm_mobilenet_ssd.cpp export LD_LIBRARY_PATH=./build/install/lib ./build/install/bin/tm_mobilenet_ssd -m models/mobilenet_ssd.tmfile -i images/ssd_dog.jpg -r 1 -t 1 Input Image: Output Image: 3. retinaface 人脸检测任务 代码地址：https://github.com/OAID/Tengine/blob/tengine-lite/examples/tm_refinaface.cpp export LD_LIBRARY_PATH=./build/install/lib ./build/install/bin/tm_retinaface -m models/retinaface.tmfile -i images/mtcnn_face4.jpg -r 1 -t 1 Input Image: Output Image: 4. yolact 实例分割任务 代码地址：https://github.com/OAID/Tengine/blob/tengine-lite/examples/tm_yolact.cpp export LD_LIBRARY_PATH=./build/install/lib ./build/install/bin/tm_yolact -m models/yolact.tmfile -i images/ssd_car.jpg -r 1 -t 1 Input Image: Output Image: 5. 人体姿态识别任务 代码地址：https://github.com/OAID/Tengine/blob/tengine-lite/examples/tm_openpose.cpp export LD_LIBRARY_PATH=./build/install/lib ./build/install/bin/tm_openpose -m models/openpose_coco.tmfile -i image/pose.jpg -r 1 -t 1 Input Image: Output Image1: Output Image2: "},"ReasoningEngine/reasoningengine.html":{"url":"ReasoningEngine/reasoningengine.html","title":"推理引擎","keywords":"","body":"推理引擎 使用Tengine-Lite流程 编译对应Tengine-Lite库 -> libtengine-lite.so -> 在代码中运行 "},"ReasoningEngine/compile.html":{"url":"ReasoningEngine/compile.html","title":"编译","keywords":"","body":"编译 环境 cmake (建议使用3.10或以上版本) protobuf (建议使用3.0或以上版本) gcc (建议使用4.9或以上版本) Ubuntu 18.04 环境安装命令： sudo apt-get install cmake make g++ git Fedora28 环境安装命令： sudo dnf install cmake make g++ git 下载源码 git clone https://github.com/OAID/Tengine.git "},"ReasoningEngine/compilecmakelists.html":{"url":"ReasoningEngine/compilecmakelists.html","title":"编译CMakeLists参数","keywords":"","body":"编译CMakeLists参数 CMakeLists 选项 参数 默认值 做用 TENGINE_OPENMP ON 支持openmp TENGINE_BUILD_BENCHMARK ON 编译benchmark TENGINE_BUILD_EXAMPLES ON 编译examples TENGINE_BUILD_TESTS OFF 编译tests TENGINE_BUILD_CPP_API ON 编译C++ API TENGINE_DEBUG_DATA OFF 打印每个层数据 TENGINE_DEBUG_TIME OFF 打印每层时间信息 TENGINE_DEBUG_MEM_STAT OFF 打印内存状态 TENGINE_ARCH_X86_AVX ON 编译avx2 for x86 TENGINE_ARCH_ARM_82 OFF 编译armv8.2 for arm 插件选项 参数 默认值 做用 TENGINE_ENABLE_ACL OFF 使用Arm Compute Library（ACL）支持进行构建 TENGINE_ENABLE_VULKAN OFF 使用Vulkan GPU计算支持进行构建 "},"ReasoningEngine/androidreasoning.html":{"url":"ReasoningEngine/androidreasoning.html","title":"编译Android推理库","keywords":"","body":"编译Android推理库 本地编译 在https://developer.android.com/ndk/downloads/下载安装NDK，建议使用最新稳定版本。 配置环境变量：export ANDROID_NDK=/Users/username/path/to/android-ndk-r16b。 编译armv7动态库： mkdir build-android-v7a cd build-android-v7a cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=\"armeabi-v7a\" \\ -DANDROID_PLATFORM=android-21 .. make -j8 cd .. 编译armv8动态库： mkdir build-android-v8a cd build-android-v8a cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=\"arm64-v8a\" \\ -DANDROID_PLATFORM=android-21 .. make -j8 cd .. "},"ReasoningEngine/linuxreasoning.html":{"url":"ReasoningEngine/linuxreasoning.html","title":"编译Linux推理库","keywords":"","body":"编译Linux推理库 本地编译 mkdir build && cd build && cmake .. && make -j8 交叉编译 交叉编译，由于目前各大厂商各不相同，大致有两个步骤（以Linaro ARM64为例子）： 获取交叉编译工具链(以ARM64为例子) 到https://releases.linaro.org/components/toolchain/binaries/latest-7/网站下载工具链。 下载gcc-linaro-7.5.0-2019.12-x86_64_arm-linux-gnueabi.tar.xz。 解压并存放。 利用工具链 mkdir build-aarch64-linux-gnu cd build-aarch64-linux-gnu cmake -DCMAKE_TOOLCHAIN_FILE=工具链所在位置/toolchains/aarch64-linux-gnu.toolchain.cmake .. make -j4 && make install cd .. "},"ReasoningEngine/reasoning.html":{"url":"ReasoningEngine/reasoning.html","title":"推理","keywords":"","body":"推理 Tengine-Lite推理过程 初始化 -> prerun -> 创建会话 -> 输入数据 -> 推理 -> 得到输出 -> 释放 "},"ReasoningEngine/inittengine.html":{"url":"ReasoningEngine/inittengine.html","title":"初始化","keywords":"","body":"Tengine初始化 初始化 /*! * @brief Initialize the tengine, only can be called once. * * @return 0: Success, -1: Fail. */ int init_tengine(void); Example: init_tengine(); 获取Tengine版本 /*! * @brief Check the run-time library supports the verson. * app developer should call get_tengine_version() to save the version used * during developping. * * this interface is designed for app built with dynamic tengine library. * The app knows exactly that it can work on a tengine version, and it can * check run-time tengine library supports that version. * * @param [in] version: A c string returned by get_tengine_version() * @return 1: support, 0: not support. */ int request_tengine_version(const char* version); Example: if (request_tengine_version(\"1.0\") "},"ReasoningEngine/createsession.html":{"url":"ReasoningEngine/createsession.html","title":"创建会话","keywords":"","body":"创建会话 创建Graph 通过模型路径创建模型持有者： /*! * @brief Create the run-time graph for execution from a saved model. * If model format is NULL, an empty graph handle will be returned. * * @param [in] context: The context the graph will run inside; * could be NULL and the graph is created in a private context. * * @param [in] model_format: The model format type,such as \"caffe\",\"tengine\" * @param [in] file_name: The name of model file. * * @return The graph handler or NULL if failed. */ graph_t create_graph(context_t context, const char* model_format, const char* file_name, ...); Example: graph_t graph = create_graph(nullptr, \"tengine\", model_path.c_str()); 配置选项（多线程才需要配置） /*! * @brief Initialize resource for graph execution, and set cluster and threads count will used. * * @param [in] graph: The graph handle. * @param [in] cluster: The wanted cluster of all cpu clusters. * @param [in] threads: The threads count of graph will used to run. * * @return 0: Success, -1: Fail. * */ struct options opt; opt.num_thread = num_thread; opt.cluster = TENGINE_CLUSTER_ALL; opt.precision = TENGINE_MODE_FP32; 创建输入Tensor 有两种方式创建： 只有一个输入的时候： /*! * @brief Get tensor handle of one graph input tensor. * * @param [in] graph: The graph handle. * @param [in] input_node_idx: The input node index, starting from zero. * @param [in] tensor_idx: The output tensor index of the input node, starting from zero. * * @return The tensor handle or NULL on error. */ tensor_t get_graph_input_tensor(graph_t graph, int input_node_idx, int tensor_idx); Example: tensor_t input_tensor = get_graph_input_tensor(graph, 0, 0); 有多个输入的时候,用ID获取： /*! * @brief Get a tensor handle by tensor name. * * @param [in] graph: The graph handle. * @param [in] tensor_name: Tensor name. * * @return The tensor handle or NULL on error. * */ tensor_t get_graph_tensor(graph_t graph, const char* tensor_name); Example: tensor_t input_tensor = get_graph_tensor(graph, \"input\"); 设置Tensor形状 /*! * @brief Set the shape of tensor. * * @param [in] tensor: The tensor handle. * @param [in] dims: An int array to represent shape. * @param [in] dim_number: The array size. * @return 0: Success; -1: Fail. * */ int set_tensor_shape(tensor_t tensor, const int dims[], int dim_number); Example: int dims[] = {1, model_input_channel, model_input_height, model_input_widht}; // NCHW set_tensor_shape(input_tensor, dims, 4); 预运行Tengine(一定要有！！) 有单线程和多线程两种方式： 单线程 /*! * @brief Initialize resource for graph execution. * * @param [in] graph: The graph handle. * * @return 0: Success, -1: Fail. * */ int prerun_graph(graph_t graph); Example: if (prerun_graph(graph) != 0) { std::cout 多线程 /*! * @brief Initialize resource for graph execution, and set cluster and threads count will used. * * @param [in] graph: The graph handle. * @param [in] cluster: The wanted cluster of all cpu clusters. * @param [in] threads: The threads count of graph will used to run. * * @return 0: Success, -1: Fail. * */ int prerun_graph_multithread(graph_t graph, struct options opt); Example: if (prerun_graph_multithread(graph, opt) != 0){ std::cout "},"ReasoningEngine/inputdata.html":{"url":"ReasoningEngine/inputdata.html","title":"输入数据","keywords":"","body":"输入数据 对输入图像进行格式转换（NCHW->NHWC） int hw = image_widht * image_height; int index = 0; for (int w = 0; w 对输入图像进行格式转换并归一化（NCHW->NHWC） int hw = image_widht * image_height; int index = 0; for (int w = 0; w 设置输入 /*! * @brief Set the buffer of the tensor. * A tensor may deny to change its internal buffer setting. * * @param [in] tensor: The tensor handle. * @param [in] buffer: The buffer address. * @param [in] buffer_size: The buffer_size. * * @return 0: Success; -1: Fail. * @note The buffer is still managed by caller. */ int set_tensor_buffer(tensor_t tensor, void* buffer, int buffer_size); 完整代码 // 输入数据 float* send_im = new float[image_width * image_height * image_channel]; int hw = image_width * image_height; int index = 0; // NCHW->NHWC for (int w = 0; w "},"ReasoningEngine/runsession.html":{"url":"ReasoningEngine/runsession.html","title":"推理会话","keywords":"","body":"推理会话 运行 /*! * @brief Execute graph. * * @param [in] graph: The graph handle. * @param [in] block: Blocking or nonlocking. * @return 0: Success, -1: Fail. * @note If block is 0, need to call wait_graph to get result or set GRAPH_DONE event hook. * */ int run_graph(graph_t graph, int block); Example: run_graph(graph, 1); "},"ReasoningEngine/outputdata.html":{"url":"ReasoningEngine/outputdata.html","title":"得到输出","keywords":"","body":"得到输出 **获取Tensor输出 /*! * @brief Get a tensor handle by tensor name. * * @param [in] graph: The graph handle. * @param [in] tensor_name: Tensor name. * * @return The tensor handle or NULL on error. * */ tensor_t get_graph_tensor(graph_t graph, const char* tensor_name); /*! * @brief Get a tensor handle of a graph output node. * * @param [in] graph: The graph handle. * @param [in] output_node_idx: The output node index. * @param [in] tensor_idx: The output tensor index of the output node. * * @return The tensor handle or NULL on error. * */ tensor_t get_graph_output_tensor(graph_t graph, int output_node_idx, int tensor_idx); /*! * @brief Get the buffer of the tensor. * A tensor may deny to expose its internal buffer, so that get_tensor_buffer() * will fail but get_tensor_buffer_size()/set_tensor_data() succeed. * * @param [in] tensor: The tensor handle. * @return The buffer address. if no buffer allocated return NULL. */ void* get_tensor_buffer(tensor_t tensor); Example: tensor_t mTensor = get_graph_tensor(graph, \"Tensor_output_name\"); float *outputData = (float*)get_tensor_buffer(mTensor); 或者： tensor_t mTensor = get_graph_output_tensor(graph, 0, 0); float *outputData = (float*)get_tensor_buffer(mTensor); "},"ReasoningEngine/release.html":{"url":"ReasoningEngine/release.html","title":"释放内存","keywords":"","body":"释放内存 释放 在你不需要的时候需要进行释放，避免内存泄漏。 /*! * @brief Release the tensor handle. * * @param [in] tensor: The tensor handle. * * @return None. */ void release_graph_tensor(tensor_t tensor); /*! * @brief Release the resource for graph execution. * @param [in] graph: graph handle. * * @return 0: Success, -1: Fail. */ int postrun_graph(graph_t graph); /*! * @brief Destory the runtime graph and release allocated resource. * * @param [in] graph: The graph handle. * @return 0: Success, -1: Fail. */ int destroy_graph(graph_t graph); /*! * @brief Release the tengine, only can be called once. * * @return none. */ void release_tengine(void); Example: release_graph_tensor(input_tensor); postrun_graph(graph); destroy_graph(graph); release_tengine(); "},"Transformation/transfertool.html":{"url":"Transformation/transfertool.html","title":"转换工具","keywords":"","body":"转换工具 使用Tengine-Lite模型转换工具流程 xxx.model -> xxx.tmfile "},"Transformation/compile.html":{"url":"Transformation/compile.html","title":"编译","keywords":"","body":"编译 环境 cmake (建议使用3.10或以上版本) protobuf (建议使用3.0或以上版本) gcc (建议使用4.9或以上版本) Ubuntu 18.04 环境安装命令： sudo apt-get install cmake make g++ git sudo apt install libprotobuf-dev protobuf-compiler Fedora28 环境安装命令： sudo dnf install cmake make g++ git sudo dnf install protobuf-devel sudo dnf install boost-devel glog-devel 下载源码 git clone https://github.com/OAID/Tengine-Convert-Tools.git "},"Transformation/linuxtransfer.html":{"url":"Transformation/linuxtransfer.html","title":"转换工具","keywords":"","body":"编译Linux转换工具 总共有3种转换方式： 下载已经编译好的编译工具tm_convert_tool，在Linux上运行。 网站在线转换。 编译源码进行转换 git clone https://github.com/OAID/Tengine-Convert-Tools.git mkdir build cd build cmake .. && make -j4 "},"Transformation/transfer.html":{"url":"Transformation/transfer.html","title":"模型转换","keywords":"","body":"转换 转换指令 Caffe ./install/bin/tm_convert_tool -f caffe -p mobilenet_deploy.prototxt -m mobilenet.caffemodel -o mobilenet.tmfile MxNet ./install/bin/tm_convert_tool -f mxnet -p mobilenet1_0-symbol.json -m mobilene1_0-0000.params -o mobileent.tmfile ONNX ./install/bin/tm_convert_tool -f onnx -m mobilenet.onnx -o mobilenet.tmfile TensorFlow ./install/bin/tm_convert_tool -f tensorflow -m mobielenet_v1_1.0_224_frozen.pb -o mobilenet.tmfile TFLITE ./install/bin/tm_convert_tool -f tflite -m mobielenet.tflite -o mobilenet.tmfile DarkNet: darknet only support for yolov3 model ./install/bin/tm_convert_tool -f darknet -p yolov3.cfg -m yolov3.weights -o yolov3.tmfile NCNN ./install/bin/tm_convert_tool -f ncnn -p mobilenet.params -m mobilenet.bin -o mobilenet.tmfile MegEngine ./install/bin/tm_convert_tool -f megengine -m mobilenet.pkl -o mobilenet.tmfile "}}