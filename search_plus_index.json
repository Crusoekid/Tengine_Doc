{"./":{"url":"./","title":"介绍","keywords":"","body":"Tengine-Lite Tengine Lite 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目基于原有 Tengine 项目使用 C 语言进行重构，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署。同时兼容 Tengine 框架原有 API 和 模型格式 tmfile，降低评估、迁移成本。 "},"Introduction/about.html":{"url":"Introduction/about.html","title":"关于","keywords":"","body":"关于 Tengine Lite 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目基于原有 Tengine 项目使用 C 语言进行重构，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署。同时兼容 Tengine 框架原有 API 和 模型格式 tmfile，降低评估、迁移成本。 Tengine Lite 核心代码由 4 个模块组成： dev：NN Operators 后端模块，当前提供 CPU 代码，后续逐步开源 GPU、NPU 参考代码； lib：框架核心部件，包括 NNIR、计算图、硬件资源、模型解析器的调度和执行模块； op：NN Operators 前端模块，实现 NN Operators 注册、初始化； serializer：模型解析器，实现 tmfile 格式的网络模型参数解析。 "},"Introduction/feedback.html":{"url":"Introduction/feedback.html","title":"反馈","keywords":"","body":"反馈 提交Issue 你可以提交issue给我们。 你不仅可以谈论还能赚钱哦 赏金任务第一期 赏金任务第二期 加入群聊 QQ群（集中地）： 微信群： "},"ReasoningEngine/reasoningengine.html":{"url":"ReasoningEngine/reasoningengine.html","title":"推理引擎","keywords":"","body":"推理引擎 使用Tengine-Lite流程 编译对应Tengine-Lite库 -> libtengine-lite.so -> 在代码中运行 "},"ReasoningEngine/compile.html":{"url":"ReasoningEngine/compile.html","title":"编译","keywords":"","body":"编译 环境 cmake (建议使用3.10或以上版本) protobuf (建议使用3.0或以上版本) gcc (建议使用4.9或以上版本) Ubuntu 18.04 环境安装命令： sudo apt-get install cmake make g++ git Fedora28 环境安装命令： sudo dnf install cmake make g++ git 下载源码 git clone https://github.com/OAID/Tengine.git "},"ReasoningEngine/androidreasoning.html":{"url":"ReasoningEngine/androidreasoning.html","title":"编译Android推理库","keywords":"","body":"编译Android推理库 本地编译 在https://developer.android.com/ndk/downloads/下载安装NDK，建议使用最新稳定版本。 配置环境变量：export ANDROID_NDK=/Users/username/path/to/android-ndk-r16b。 编译armv7动态库： mkdir build-android-v7a cd build-android-v7a cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=\"armeabi-v7a\" \\ -DANDROID_PLATFORM=android-21 .. make -j8 cd .. 编译armv8动态库： mkdir build-android-v8a cd build-android-v8a cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=\"arm64-v8a\" \\ -DANDROID_PLATFORM=android-21 .. make -j8 cd .. "},"ReasoningEngine/linuxreasoning.html":{"url":"ReasoningEngine/linuxreasoning.html","title":"编译Linux推理库","keywords":"","body":"编译Linux推理库 本地编译 mkdir build && cd build && cmake .. && make -j8 交叉编译 交叉编译，由于目前各大厂商各不相同，大致有两个步骤（以Linaro ARM64为例子）： 获取交叉编译工具链(以ARM64为例子) 到https://releases.linaro.org/components/toolchain/binaries/latest-7/网站下载工具链。 下载gcc-linaro-7.5.0-2019.12-x86_64_arm-linux-gnueabi.tar.xz。 解压并存放。 利用工具链 mkdir build-aarch64-linux-gnu cd build-aarch64-linux-gnu cmake -DCMAKE_TOOLCHAIN_FILE=工具链所在位置/toolchains/aarch64-linux-gnu.toolchain.cmake .. make -j4 && make install cd .. "},"ReasoningEngine/reasoning.html":{"url":"ReasoningEngine/reasoning.html","title":"推理","keywords":"","body":"推理 Tengine-Lite推理过程 初始化 -> prerun -> 创建会话 -> 输入数据 -> 推理 -> 得到输出 -> 释放 "},"ReasoningEngine/inittengine.html":{"url":"ReasoningEngine/inittengine.html","title":"初始化","keywords":"","body":"Tengine初始化 初始化 /*! * @brief Initialize the tengine, only can be called once. * * @return 0: Success, -1: Fail. */ init_tengine(); 获取Tengine版本 /*! * @brief Check the run-time library supports the verson. * app developer should call get_tengine_version() to save the version used * during developping. * * this interface is designed for app built with dynamic tengine library. * The app knows exactly that it can work on a tengine version, and it can * check run-time tengine library supports that version. * * @param [in] version: A c string returned by get_tengine_version() * @return 1: support, 0: not support. */ if (request_tengine_version(\"1.0\") "},"ReasoningEngine/createsession.html":{"url":"ReasoningEngine/createsession.html","title":"创建会话","keywords":"","body":"创建会话 创建Graph 通过模型路径创建模型持有者： /*! * @brief Create the run-time graph for execution from a saved model. * If model format is NULL, an empty graph handle will be returned. * * @param [in] context: The context the graph will run inside; * could be NULL and the graph is created in a private context. * * @param [in] model_format: The model format type,such as \"caffe\",\"tengine\" * @param [in] file_name: The name of model file. * * @return The graph handler or NULL if failed. */ graph_t graph = create_graph(nullptr, \"tengine\", model_path.c_str()); 配置选项（多线程才需要配置） /*! * @brief Initialize resource for graph execution, and set cluster and threads count will used. * * @param [in] graph: The graph handle. * @param [in] cluster: The wanted cluster of all cpu clusters. * @param [in] threads: The threads count of graph will used to run. * * @return 0: Success, -1: Fail. * */ struct options opt; opt.num_thread = num_thread; opt.cluster = TENGINE_CLUSTER_ALL; opt.precision = TENGINE_MODE_FP32; if (prerun_graph_multithread(graph, opt) != 0){ std::cout 创建输入Tensor 有两种方式创建： 只有一个输入的时候： /*! * @brief Get tensor handle of one graph input tensor. * * @param [in] graph: The graph handle. * @param [in] input_node_idx: The input node index, starting from zero. * @param [in] tensor_idx: The output tensor index of the input node, starting from zero. * * @return The tensor handle or NULL on error. */ tensor_t input_tensor = get_graph_input_tensor(graph, 0, 0); 有多个输入的时候： /*! * @brief Get a tensor handle by tensor name. * * @param [in] graph: The graph handle. * @param [in] tensor_name: Tensor name. * * @return The tensor handle or NULL on error. * */ tensor_t input_tensor = get_graph_tensor(graph, \"input\"); 设置Tensor形状 int dims[] = {1, model_input_channel, model_input_height, model_input_widht}; // NCHW /*! * @brief Set the shape of tensor. * * @param [in] tensor: The tensor handle. * @param [in] dims: An int array to represent shape. * @param [in] dim_number: The array size. * @return 0: Success; -1: Fail. * */ set_tensor_shape(input_tensor, dims, 4); 预运行Tengine(一定要有！！) 有单线程和多线程两种方式： 单线程 /*! * @brief Initialize resource for graph execution. * * @param [in] graph: The graph handle. * * @return 0: Success, -1: Fail. * */ if (prerun_graph(graph) != 0) { std::cout 多线程 /*! * @brief Initialize resource for graph execution, and set cluster and threads count will used. * * @param [in] graph: The graph handle. * @param [in] cluster: The wanted cluster of all cpu clusters. * @param [in] threads: The threads count of graph will used to run. * * @return 0: Success, -1: Fail. * */ if (prerun_graph_multithread(graph, opt) != 0){ std::cout "},"ReasoningEngine/inputdata.html":{"url":"ReasoningEngine/inputdata.html","title":"输入数据","keywords":"","body":"输入数据 对输入图像进行格式转换（NCHW->NHWC） int hw = image_widht * image_height; int index = 0; for (int w = 0; w 对输入图像进行格式转换并归一化（NCHW->NHWC） int hw = image_widht * image_height; int index = 0; for (int w = 0; w 设置输入 /*! * @brief Set the buffer of the tensor. * A tensor may deny to change its internal buffer setting. * * @param [in] tensor: The tensor handle. * @param [in] buffer: The buffer address. * @param [in] buffer_size: The buffer_size. * * @return 0: Success; -1: Fail. * @note The buffer is still managed by caller. */ set_tensor_buffer(input_tensor, send_im, image_width * image_height * image_channel * sizeof(float)); 完整代码 // 输入数据 float* send_im = new float[image_width * image_height * image_channel]; int hw = image_width * image_height; int index = 0; // NCHW->NHWC for (int w = 0; w "},"ReasoningEngine/runsession.html":{"url":"ReasoningEngine/runsession.html","title":"推理会话","keywords":"","body":"推理会话 运行 /*! * @brief Execute graph. * * @param [in] graph: The graph handle. * @param [in] block: Blocking or nonlocking. * @return 0: Success, -1: Fail. * @note If block is 0, need to call wait_graph to get result or set GRAPH_DONE event hook. * */ run_graph(graph, 1); "},"ReasoningEngine/outputdata.html":{"url":"ReasoningEngine/outputdata.html","title":"得到输出","keywords":"","body":"得到输出 **获取Tensor输出 /*! * @brief Get a tensor handle by tensor name. * * @param [in] graph: The graph handle. * @param [in] tensor_name: Tensor name. * * @return The tensor handle or NULL on error. * */ tensor_t mTensor = get_graph_tensor(graph, \"Tensor_output_name\"); /*! * @brief Get the buffer of the tensor. * A tensor may deny to expose its internal buffer, so that get_tensor_buffer() * will fail but get_tensor_buffer_size()/set_tensor_data() succeed. * * @param [in] tensor: The tensor handle. * @return The buffer address. if no buffer allocated return NULL. */ float *outputData = (float*)get_tensor_buffer(mTensor); "},"ReasoningEngine/release.html":{"url":"ReasoningEngine/release.html","title":"释放内存","keywords":"","body":"释放内存 释放 在你不需要的时候需要进行释放，避免内存泄漏。 /*! * @brief Release the tensor handle. * * @param [in] tensor: The tensor handle. * * @return None. */ release_graph_tensor(input_tensor); /*! * @brief Release the resource for graph execution. * @param [in] graph: graph handle. * * @return 0: Success, -1: Fail. */ postrun_graph(graph); /*! * @brief Destory the runtime graph and release allocated resource. * * @param [in] graph: The graph handle. * @return 0: Success, -1: Fail. */ destroy_graph(graph); /*! * @brief Release the tengine, only can be called once. * * @return none. */ release_tengine(); "},"Transformation/transfertool.html":{"url":"Transformation/transfertool.html","title":"转换工具","keywords":"","body":"转换工具 使用Tengine-Lite模型转换工具流程 xxx.model -> xxx.tmfile "},"Transformation/compile.html":{"url":"Transformation/compile.html","title":"编译","keywords":"","body":"编译 环境 cmake (建议使用3.10或以上版本) protobuf (建议使用3.0或以上版本) gcc (建议使用4.9或以上版本) Ubuntu 18.04 环境安装命令： sudo apt-get install cmake make g++ git sudo apt install libprotobuf-dev protobuf-compiler Fedora28 环境安装命令： sudo dnf install cmake make g++ git sudo dnf install protobuf-devel sudo dnf install boost-devel glog-devel 下载源码 git clone https://github.com/OAID/Tengine-Convert-Tools.git "},"Transformation/linuxtransfer.html":{"url":"Transformation/linuxtransfer.html","title":"转换工具","keywords":"","body":"编译Linux转换工具 总共有3种转换方式： 下载已经编译好的编译工具tm_convert_tool，在Linux上运行。 网站在线转换。 编译源码进行转换 git clone https://github.com/OAID/Tengine-Convert-Tools.git mkdir build cd build cmake .. && make -j4 "},"Transformation/transfer.html":{"url":"Transformation/transfer.html","title":"模型转换","keywords":"","body":"转换 转换指令 Caffe ./install/bin/tm_convert_tool -f caffe -p mobilenet_deploy.prototxt -m mobilenet.caffemodel -o mobilenet.tmfile MxNet ./install/bin/tm_convert_tool -f mxnet -p mobilenet1_0-symbol.json -m mobilene1_0-0000.params -o mobileent.tmfile ONNX ./install/bin/tm_convert_tool -f onnx -m mobilenet.onnx -o mobilenet.tmfile TensorFlow ./install/bin/tm_convert_tool -f tensorflow -m mobielenet_v1_1.0_224_frozen.pb -o mobilenet.tmfile TFLITE ./install/bin/tm_convert_tool -f tflite -m mobielenet.tflite -o mobilenet.tmfile DarkNet: darknet only support for yolov3 model ./install/bin/tm_convert_tool -f darknet -p yolov3.cfg -m yolov3.weights -o yolov3.tmfile NCNN ./install/bin/tm_convert_tool -f ncnn -p mobilenet.params -m mobilenet.bin -o mobilenet.tmfile MegEngine ./install/bin/tm_convert_tool -f megengine -m mobilenet.pkl -o mobilenet.tmfile "}}